{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e992e5f0f6e0a177efc20ce2f6a4aa1f",
     "grade": false,
     "grade_id": "cell-40860fed0967cc2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Assignment 1: NLP Basics \n",
    " \n",
    " _Solution will be submitted by May 6, 23:59._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2417dac730a8e25ffacd8356904a8d0f",
     "grade": false,
     "grade_id": "cell-fc97745f1a1c55e0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 1  (2 points: 0.5 + 0.5 + 0.5 + 0.5)\n",
    "\n",
    "Mark which of the following sentences are correct and provide one or two sentences as an explanation for your answer:\n",
    "\n",
    "a) Regularization favors complex models more than simpler models.\n",
    "\n",
    "b) Topic modeling should be modeled as a hard clustering task.\n",
    "\n",
    "c) Part of speech tags can be modeled with an ordinal variable.\n",
    "\n",
    "d) Accuracy is a suitable metric for quantifying how effective an approach in tasks with an unbalanced class distribution (i.e., when there is a clearly over-represented majority class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebd4da3f637b27ea289b9ce5274cdad1",
     "grade": true,
     "grade_id": "cell-3b09d1bd9eae5c5c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a614278f39abee2928f2cf6ff069f28",
     "grade": false,
     "grade_id": "cell-615003803b803ed7",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2 (3 points: 1 + 2)\n",
    "\n",
    "You are asked to develop a classifier that detects spam emails. You model the task as follows: given an email it is labeled with 1 if it is a spam, and 0 if it is ham. You know that only 1 % of the emails are actually spam. Given the two variants of $F_{1}\\mbox{-}score$, $micro\\mbox{ }F_{1}\\mbox{-}score$ and $macro \\mbox{ }F_{1}\\mbox{-}score$ that can be computed using the following equation. Given a split $D$ that contains $N$ instances, the variables $c_{1}, c_{2}, \\dots, c_{M}$ that represent the size of the instances in the split that are labeled with the classes ${1, 2, \\dots, M}$. $micro\\mbox{ }F_{1}\\mbox{-}score$ and $macro \\mbox{ }F_{1}\\mbox{-}score$ can be computed as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$ micro\\mbox{ }F_{1}\\textrm{-score} = \\sum_{i=1}^{M}\\frac{c_{i}}{D}{F_{1}^{i}}\\mbox{-}score$$\n",
    "$$ macro\\mbox{ }F_{1}\\textrm{-score} = \\sum_{i=1}^{M}\\frac{1}{M}{F_{1}^{i}}\\mbox{-}score$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251ed7634d7835dd89869997254d7d9c",
     "grade": false,
     "grade_id": "cell-3e8eee2183611eb0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "a) Given the following ground-truth and predictions of a classifier. Calculate the $micro\\mbox{ }F_{1}\\mbox{-}score$ and $macro \\mbox{ }F_{1}\\mbox{-}score$ on the given split. Please round the metrics to two decimal points. \n",
    "\n",
    "\n",
    "$$ground-truth = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1 ]$$\n",
    "$$predictions  = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]$$\n",
    "\n",
    "b) Which of the two metrics would you use for spam detection, explain your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9285922b6ff7d4b54c030339f549e2fe",
     "grade": true,
     "grade_id": "cell-e178f05aa435eaa5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd9960eb300e1274211315b3bf6bd71f",
     "grade": false,
     "grade_id": "cell-f92a048a370c6304",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3 (points 7: 1 + 1 + 3 + 1 +1)\n",
    "While developing a text summarization model, you get the following scores in a 5-fold cross-validation setting ($bleu$ stands for the scores of your model and $bleu'$ for the baseline). The $bleu$ score calculates the quality of machine-generated text using the count of common tokens in the generated text the occur in the ground-truth texts (usually a generated-text is compared against multiple candidates). You will learn about the formula of the bleu score later in the lecture. It is not needed here.\n",
    "\n",
    "$$bleu = [0.42, 0.42, 0.42, 0.43, 0.45]$$\n",
    "$$bleu' = [0.40, 0.42, 0.41, 0.42, 0.44]$$\n",
    "\n",
    "You want to show that the two models perform similarly. Since the same set of examples is subject to two different models, a dependent t-test for paired samples should be applied. Assuming that the scores are normally distributed, use a dependent t-test for paired samples with a significance level $\\alpha$ of 0.05 to find out whether the performance of the model is significantly better than the baseline. While calculating the $t$-score, please always round your decimals to four decimal points. To perform the test do the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5c6fb04a81d0d73d7c31d1fcdd66fc4",
     "grade": false,
     "grade_id": "cell-55c6b858330a0391",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Instructions\n",
    "\n",
    "a) Decide whether the test is one-tailed or two-tailed.\n",
    "\n",
    "b) Formulate the research hypothesis $H$ and the null hypothesis $H_0$.\n",
    "\n",
    "c) Calculate the $t$-score.\n",
    "\n",
    "d) Look up the $p$-value for the right degree of freedom at https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf\n",
    "\n",
    "e) Decide whether $H_0$ can be rejected. Explain your decision in 1-2 sentences.\n",
    "\n",
    "Below, we give an example. If needed, you find more details on dependent $t$-tests on the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "82619348a2856b07a67cbb3fd41c2c2e",
     "grade": false,
     "grade_id": "cell-26563eceffed2814",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "Assume that you want to show that a classifier significantly outperforms a baseline in a 5-fold cross-validation setting with the following scores (again multiplied with 100). $Accuracy$ is the scores of the model and $accuracy'$ stands for the scores of the baseline:\n",
    "\n",
    "\n",
    "$$ accuracy' = [71, 70.5, 71.8, 72, 70.3]$$\n",
    "$$ accuracy = [73, 74.2, 72.3, 73.3, 74.1]$$\n",
    "\n",
    "a) Type of test: \n",
    "Since we assume the model scores to be larger than the baseline, we perform a one-tailed paired t-test. We take a significance level $\\alpha$ of 0.05.\n",
    "\n",
    "b) Formulating hypotheses:\n",
    "\n",
    "$H$: the mean of the scores of the model is significantly larger than the mean of the baseline's scores.\n",
    "\n",
    "$H_0$: the mean of the scores of the model is not significantly larger than the baseline's scores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0450419ed66b1493529843b0b995cc40",
     "grade": false,
     "grade_id": "cell-3a51cf212168fb4c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "c) Calculate a $t$-score as follows:\n",
    "\n",
    "\n",
    "$$t =\\frac{\\bar{D}-\\mu_{D}}{\\frac{s_{D}}{\\sqrt{n}}}$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "* $\\bar{D}$ is the mean distance between the model and the baseline's scores. \n",
    "* $\\mu_{D}$ is the expected difference between the population means. \n",
    "* $s_{D}$ is the standard deviation of the scores distances.\n",
    "* $n$ is the count of scores which is five here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\mu_{D}$ is considered zero since the expected difference of the population mean when the null hypothesis applies is zero.    \n",
    "\n",
    "$$ \\bar{D} = \\frac{\\sum_{1}^{n}{(accuracy_i - accuracy'_i)}}{n} = 2.26$$ \n",
    "\n",
    "$$ s_{D}=\\sqrt{\\frac{\\sum_{1}^{n}{((accuracy_i - accuracy'_i)-\\bar{D})Â²}}{n-1}} = 1.46$$\n",
    "$$t = \\frac{2.6}{\\frac{1.46}{ \\sqrt{5}}} = 3.46$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "244b089e30a5e72ec14d61ce659057ee",
     "grade": false,
     "grade_id": "cell-d8007fa295f4b875",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "d) Look up $p$-value for the right degree of freedom:\n",
    "\n",
    "Calculating the degree of freedom. The degree of freedom is the count of values that are allowed to vary in our sample. Since we consider differences between paired scores, the degree of freedom is n - 1 in this case, which is four. \n",
    "\n",
    "Looking up the $p$-value in the table [0] by finding the largest t-value in the table that is smaller than $t$ which is in this case a $p$-value of $0.025$ for the $t$-value $2.776$.\n",
    "\n",
    "\n",
    "e) Accept or reject $H_0$.\n",
    "\n",
    "In this case we reject $H_0$ since 0.025 ($p$-value) $\\le$ 0.05 ($\\alpha$)\n",
    "\n",
    "[0]: https://www.sjsu.edu/faculty/gerstman/StatPrimer/t-table.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28ce8c40f15b062f1406c08672d0989b",
     "grade": true,
     "grade_id": "cell-e40308493a421f35",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fffa82a65c139d517cbebc03b2a39cb0",
     "grade": false,
     "grade_id": "cell-ca701cc7b0b9cc64",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 4 (points 4: 1 + 3)\n",
    "\n",
    "Given the following two sentences which are parsed using the NLTK part of speech (POS) tagger (see https://pythonexamples.org/nltk-pos-tagging/ for the complete nltk pos tag list):\n",
    "\n",
    "* Sentence 1:    All/DT men/NNS are/VBP mortal/JJ\n",
    "* Sentence 2:    Socrates/NNP is/VBZ a/DT man/NN\n",
    "* Sentence 3:    Socrates/NNP is/VBZ mortal/JJ\n",
    "\n",
    "a) Generate the POS 1-grams and POS 2-grams of each sentence. \n",
    "\n",
    "\n",
    "b) Calculate the Jaccard similarity of each pair of sentences using the part of speech 1-grams and part of speech 2-grams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4693c9c902174191fdfe7fe1d53b0afa",
     "grade": true,
     "grade_id": "cell-7b2018b2af5eb3ee",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15560607501e46577daf9156a938af33",
     "grade": false,
     "grade_id": "cell-a7d2464f2f99936c",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 5 (4 points, 1 point for each condition)\n",
    "\n",
    "One expert classified the persuasiveness of 1000 opinionated news articles in a corpus. The corpus covers articles on four basic themes: **Pandemic**, **Climate change**, **War** , and **Refugees**. The distribution of articles over the two persuasiveness classes and the four themes are as follows:\n",
    "\n",
    "| Theme\t\t|  Persuasive (1)\t| Non-persuasive |\n",
    "|---------- | ------------------| -------------- |\n",
    "| Pandemic          | 200\t\t\t| 100\t|\n",
    "| Climate change\t| 100\t\t\t| 250\t|\n",
    "|War\t\t\t    | 50\t\t\t| 100\t|\n",
    "|Refugees\t\t    | 50\t\t    |150\t|\n",
    "\n",
    "* Split the corpus into 50% training set, 25% validation set, and 25% test set, such that\n",
    "* the training set is balanced with respect to persuasiveness;\n",
    "* learning theme information on the training set does not help classifying persuasiveness on the other sets; and\n",
    "* the validation set and the test set have the same distribution both with respect to the themes and with respect to persuasiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "098e635be9782de70cbb50cc840921e0",
     "grade": true,
     "grade_id": "cell-fc269d22e8c4ccbd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
