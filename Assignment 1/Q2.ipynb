{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth labels\n",
    "ground_truth = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# Predicted labels\n",
    "predictions = [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "# Calculate true positives, false negatives, and false positives for calss 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for class 0:\n",
      "True positive: 8, False negative: 1, False positive: 0\n",
      "Recall: 0.89\n",
      "Precision: 1.0\n",
      "F1-score: 0.94\n"
     ]
    }
   ],
   "source": [
    "print(\"Values for class 0:\")\n",
    "tp_0 = sum([1 for gt, pred in zip(ground_truth, predictions) if gt == 0 and pred == 0])\n",
    "fn_0 = sum([1 for gt, pred in zip(ground_truth, predictions) if gt == 0 and pred == 1])\n",
    "fp_0 = sum([1 for gt, pred in zip(ground_truth, predictions) if gt == 1 and pred == 0])\n",
    "\n",
    "# Print true positives, false negatives, and false positives for class 0\n",
    "print(f\"True positive: {tp_0}, False negative: {fn_0}, False positive: {fp_0}\")\n",
    "# Calculate recall, precision, and F1-score for class 0\n",
    "recall_0 = tp_0 / (tp_0 + fn_0)\n",
    "precision_0 = tp_0 / (tp_0 + fp_0)\n",
    "f1Score_0 = (2 * precision_0 * recall_0) / (precision_0 + recall_0)\n",
    "\n",
    "# Print recall, precision, and F1-score for class 0\n",
    "print(f\"Recall: {round(recall_0, 2)}\")\n",
    "print(f\"Precision: {round(precision_0, 2)}\")\n",
    "print(f\"F1-score: {round(f1Score_0, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Values for class 1:\n",
      "True positive 1, False negative 0, False positive 1\n",
      "Recall 1.0\n",
      "Precision 0.5\n",
      "F1-score 0.67\n"
     ]
    }
   ],
   "source": [
    "# add a separator line\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"Values for class 1:\")\n",
    "# Calculate true positives, false negatives, and false positives for class 1\n",
    "tp_1 = sum([1 for gt, pred in zip(ground_truth, predictions) if gt == 1 and pred == 1])\n",
    "fn_1 = sum([1 for gt, pred in zip(ground_truth, predictions) if gt == 1 and pred == 0])\n",
    "fp_1 = sum([1 for gt, pred in zip(ground_truth, predictions) if gt == 0 and pred == 1])\n",
    "\n",
    "# Print true positives, false negatives, and false positives for class 1\n",
    "print(f\"True positive {tp_1}, False negative {fn_1}, False positive {fp_1}\")\n",
    "# Calculate recall, precision, and F1-score for class 1\n",
    "recall_1 = tp_1 / (tp_1 + fn_1)\n",
    "precision_1 = tp_1 / (tp_1 + fp_1)\n",
    "f1Score_1 = (2 * precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "\n",
    "# Print recall, precision, and F1-score for class 1\n",
    "print(f\"Recall {round(recall_1, 2)}\")\n",
    "print(f\"Precision {round(precision_1, 2)}\")\n",
    "print(f\"F1-score {round(f1Score_1, 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "Values for weighted micro and macro F1-scores:\n",
      "Micro F1-score: 0.91\n",
      "Macro F1-score: 0.8\n"
     ]
    }
   ],
   "source": [
    "# add a sperator line\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(\"Values for weighted micro and macro F1-scores:\")\n",
    "# multiply the F1-score of each class by the number of samples in that class, sum the results, and divide by the total number of samples\n",
    "Micro_f1 = (\n",
    "    f1Score_0 * sum([1 for gt in ground_truth if gt == 0])\n",
    "    + f1Score_1 * sum([1 for gt in ground_truth if gt == 1])\n",
    ") / len(ground_truth)\n",
    "\n",
    "# Print weighted F1-score\n",
    "print(f\"Micro F1-score: {round(Micro_f1, 2)}\")\n",
    "\n",
    "# Calculate the macro F1-score\n",
    "\n",
    "macro_f1 = (f1Score_0 + f1Score_1) / 2\n",
    "\n",
    "# Print macro F1-score\n",
    "print(f\"Macro F1-score: {round(macro_f1, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate micro and macro F1-scores using sklearn just to check if the results are correct\n",
    "# micro_f1 = f1_score(ground_truth, predictions, average='micro')\n",
    "# macro_f1 = f1_score(ground_truth, predictions, average='macro')\n",
    "\n",
    "# # Print micro and macro F1-scores\n",
    "# print(f\"Micro F1-score: {round(micro_f1, 2)}\")\n",
    "# print(f\"Macro F1-score: {round(macro_f1, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
